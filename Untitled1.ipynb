{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a815b150-d9d5-4255-9566-6e2ad07bd8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import json\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d0a6884-d75c-431b-a46f-a3d5705a4b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = r'E:\\code\\capstone\\data'\n",
    "json_file = r'E:\\code\\capstone\\project-1-at-2024-05-28-01-40-d3301fc8.json'\n",
    "#num_classes = 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e83cabda-86f8-4a55-a286-4e93360a916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, json_file, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create a set to store all unique labels\n",
    "        all_labels = set()\n",
    "        for item in self.data:\n",
    "            if 'choice' in item:\n",
    "                if isinstance(item['choice'], dict):\n",
    "                    all_labels.update(item['choice']['choices'])\n",
    "                else:\n",
    "                    all_labels.add(item['choice'])\n",
    "\n",
    "        # Create a dictionary to map labels to indices\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path_encoded = item['image'].split('?d=')[-1]\n",
    "        image_path = os.path.join(self.data_dir, urllib.parse.unquote(image_path_encoded))\n",
    "        #print(f\"Image path: {image_path}\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Create a tensor of zeros with a length equal to the total number of classes\n",
    "        num_classes = len(self.label_to_idx)\n",
    "        labels = torch.zeros(num_classes, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        # Set the corresponding elements in the tensor to 1 for the labels present\n",
    "        if 'choice' in item:\n",
    "            if isinstance(item['choice'], dict):\n",
    "                choices = item['choice']['choices']\n",
    "            else:\n",
    "                choices = [item['choice']]\n",
    "            for choice in choices:\n",
    "                label_idx = self.label_to_idx[choice]\n",
    "                labels[label_idx] = 1\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "        return image, labels\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     item = self.data[idx]\n",
    "    #     # if 'choice' not in item:\n",
    "    #     #     # Skip the item if it doesn't have a 'choice' field\n",
    "    #     #     return None\n",
    "\n",
    "    #     image_path_encoded = item['image'].split('?d=')[-1]\n",
    "    #     image_path = os.path.join(self.data_dir, urllib.parse.unquote(image_path_encoded))\n",
    "    #     image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    #     if self.transform:\n",
    "    #         image = self.transform(image)\n",
    "\n",
    "    #     # Create a tensor of zeros with a length equal to the total number of classes\n",
    "    #     num_classes = len(self.label_to_idx)\n",
    "    #     labels = torch.zeros(num_classes, dtype=torch.float32)\n",
    "\n",
    "    #     if isinstance(item['choice'], dict):\n",
    "    #         choices = item['choice']['choices']\n",
    "    #     else:\n",
    "    #         choices = [item['choice']]\n",
    "\n",
    "    #     # Check if there is more than one label\n",
    "    #     if len(choices) > 1:\n",
    "    #         # Option 1: Skip the instance with multiple labels\n",
    "    #         return None\n",
    "\n",
    "    #         # Option 2: Keep only the first label\n",
    "    #         # choice = choices[0]\n",
    "    #         # label_idx = self.label_to_idx[choice]\n",
    "    #         # labels[label_idx] = 1\n",
    "    #         # return image, labels\n",
    "\n",
    "    #     for choice in choices:\n",
    "    #         label_idx = self.label_to_idx[choice]\n",
    "    #         labels[label_idx] = 1\n",
    "\n",
    "    #     return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35788faf-0741-493f-9216-e0ec210f0b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40f9580a-7909-4de9-a48e-b2c9f3f41fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Assuming you have your original dataset\n",
    "dataset = CustomDataset(data_dir, json_file, transform=transform)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders for train, validation, and test sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e5eaea4-a881-47e6-aefd-bb6ba28b29d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "\n",
    "class MultiLabelMobileNetV3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiLabelMobileNetV3, self).__init__()\n",
    "        self.base_model = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        # Freeze pre-trained layers\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \"\"\"\n",
    "            \n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Linear(576, 1024),\n",
    "            nn.Hardswish(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, num_classes),\n",
    "            # nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9cfcbe2-4194-4a9e-9ca5-b10088158286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelMobileNetV3(\n",
       "  (base_model): MobileNetV3(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "            (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "            (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): Conv2dNormActivation(\n",
       "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=576, out_features=1024, bias=True)\n",
       "      (1): Hardswish()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=1024, out_features=106, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "num_classes = len(dataset.label_to_idx)\n",
    "model = MultiLabelMobileNetV3(num_classes)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a9a1a82-25dc-485b-9ae0-f456c8fd42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "# criterion = nn.BCELoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24a8f2a6-ad6a-453e-9f02-46deaffff781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: 0.0928\n",
      "Validation Loss: 0.0803\n",
      "Epoch 2/20\n",
      "Train Loss: 0.0444\n",
      "Validation Loss: 0.0508\n",
      "Epoch 3/20\n",
      "Train Loss: 0.0370\n",
      "Validation Loss: 0.0437\n",
      "Epoch 4/20\n",
      "Train Loss: 0.0296\n",
      "Validation Loss: 0.0364\n",
      "Epoch 5/20\n",
      "Train Loss: 0.0240\n",
      "Validation Loss: 0.0383\n",
      "Epoch 6/20\n",
      "Train Loss: 0.0194\n",
      "Validation Loss: 0.0310\n",
      "Epoch 7/20\n",
      "Train Loss: 0.0158\n",
      "Validation Loss: 0.0336\n",
      "Epoch 8/20\n",
      "Train Loss: 0.0128\n",
      "Validation Loss: 0.0357\n",
      "Epoch 9/20\n",
      "Train Loss: 0.0109\n",
      "Validation Loss: 0.0367\n",
      "Epoch 10/20\n",
      "Train Loss: 0.0096\n",
      "Validation Loss: 0.0364\n",
      "Epoch 11/20\n",
      "Train Loss: 0.0073\n",
      "Validation Loss: 0.0402\n",
      "Early stopping after 11 epochs\n",
      "----------\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "early_stop_count = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataloader.dataset)\n",
    "    print(f'Train Loss: {train_loss:.4f}')\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item() * images.size(0)\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataloader.dataset)\n",
    "    print(f'Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "        if early_stop_count >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs')\n",
    "            break\n",
    "\n",
    "print('-' * 10)\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f17ed3a-ec8d-4979-8a7c-87e8b2c2e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mobilenetv3-nofreeze-105-loss.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045b65ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e393c-e461-41e9-b27c-ee3ecbc6693e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('mobilenetv3-nofreeze-105-loss.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fff82099-3d7b-41ed-9a8c-4fc7fae56978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yaong\\anaconda3\\envs\\label-studio\\Lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import average_precision_score, hamming_loss\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Get probabilities and binary predictions\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            # preds = (probs > 0.5).float()\n",
    "            preds = torch.zeros_like(probs)\n",
    "            max_indices = torch.argmax(probs, dim=1, keepdim=True)\n",
    "            preds.scatter_(1, max_indices, 1.0)\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy().argmax(axis=1))\n",
    "            pred_labels.extend(preds.cpu().numpy().argmax(axis=1))\n",
    "            # true_labels.extend(labels.cpu().numpy())\n",
    "            # pred_labels.extend(preds.cpu().numpy())\n",
    "            \n",
    "\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    \n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "\n",
    "    # Calculate Hamming loss\n",
    "    hamming_loss_value = hamming_loss(all_labels, all_preds)\n",
    "\n",
    "    # Calculate mAP\n",
    "    is_multi_label = len(all_probs.shape) > 1\n",
    "    if is_multi_label:\n",
    "        num_classes = all_probs.shape[1]\n",
    "        ap_scores = []\n",
    "        for c in range(num_classes):\n",
    "            ap_scores.append(average_precision_score(all_labels[:, c], all_probs[:, c]))\n",
    "        mean_ap = np.mean(ap_scores)\n",
    "    else:\n",
    "        mean_ap = average_precision_score(all_labels, all_probs)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels.ravel(), all_preds.ravel())\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(f'Hamming Loss: {hamming_loss_value:.4f}')\n",
    "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}')\n",
    "    print(f'Mean Average Precision (mAP): {mean_ap:.4f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    skplt.metrics.plot_confusion_matrix(true_labels, pred_labels, figsize=(50, 50))\n",
    "    plt.show()\n",
    "\n",
    "# Test the model\n",
    "test(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c97e42b-2afe-441b-9bea-0390559159f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AA72': 0, 'KAI': 1, 'MK2': 2, 'MK99': 3, '가람': 4, '가브리엘': 5, '그레모리': 6, '기르가스': 7, '꼬마 공주': 8, '나리': 9, '남기사': 10, '네바': 11, '녹시아': 12, '돌프': 13, '라나': 14, '라피스': 15, '란': 16, '란팡': 17, '레아': 18, '레이': 19, '레이첼': 20, '로레인': 21, '로제타': 22, '루': 23, '루리': 24, '루시': 25, '루피나': 26, '리리스': 27, '리사': 28, '리에': 29, '린': 30, '마리나': 31, '마리아': 32, '마리안': 33, '마빈': 34, '메이릴': 35, '모리안': 36, '미래 공주': 37, '미래 기사': 38, '미래공주': 39, '미스 크롬': 40, '미야': 41, '바리': 42, '발렌시아': 43, '베로니카': 44, '베스': 45, '벨리알': 46, '비슈바크': 47, '비앙카': 48, '산타': 49, '샤피라': 50, '샬롯': 51, '세실': 52, '소히': 53, '스미레': 54, '스칼렛': 55, '시아': 56, '신틸라': 57, '아가사': 58, '아라': 59, '아라벨': 60, '아오바': 61, '아이리스': 62, '아이샤': 63, '아카유키': 64, '안드라스': 65, '안드로이드': 66, '알레프': 67, '에리나': 68, '에바': 69, '에이미': 70, '에일리': 71, '엔지': 72, '엘레노아': 73, '엘비라': 74, '엠마': 75, '여기사': 76, '오그마': 77, '오딜': 78, '오르카': 79, '요정': 80, '유나': 81, '유즈': 82, '유진': 83, '은하': 84, '카리나': 85, '카마엘': 86, '카밀라': 87, '칸나': 88, '캐럴': 89, '캐서린': 90, '코코': 91, '크레이그': 92, '크로셀': 93, '클라라': 94, '클로드': 95, '티니아': 96, '파르바티': 97, '파비': 98, '파이몬': 99, '프리실라': 100, '플리트비체': 101, '피노': 102, '하나': 103, '하얀 야수': 104, '하얀아이': 105}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e17a3-5909-4092-a1c9-49a027efe8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"E:\\code\\capstone\\data\\pixiv\\95604048\\95604048_p24.png\"\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Apply the same transformations you used during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "preprocessed_image = transform(image).unsqueeze(0)  # Add a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418862c5-a256-40c1-badf-2ad3f9dafaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(preprocessed_image.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d41ac3-89fd-4ca5-8fed-8cf74eb41073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: tensor([], device='cuda:0', size=(0, 2), dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "predicted_labels = (outputs > threshold).nonzero().squeeze(1)\n",
    "print(f\"Predicted Labels: {predicted_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a7d0ce-a7ca-4f01-9203-96381c7b5fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensor to list of tuples containing (row_index, column_index)\n",
    "predicted_labels_list = predicted_labels.tolist()\n",
    "\n",
    "# Iterate over each prediction and print the corresponding label name\n",
    "for row, col in predicted_labels_list:\n",
    "    if col in dataset.label_to_idx.values():\n",
    "        label_name = next(key for key, value in dataset.label_to_idx.items() if value == col)\n",
    "        print(f\"Predicted Label: {label_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b7a08-bd41-42df-b3d2-5a24ee9f5b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Layer:\n",
      "0.weight: True\n",
      "0.bias: True\n",
      "3.weight: True\n",
      "3.bias: True\n",
      "\n",
      "Convolutional Layers:\n",
      "0.0.weight: False\n",
      "0.1.weight: False\n",
      "0.1.bias: False\n",
      "1.block.0.0.weight: False\n",
      "1.block.0.1.weight: False\n",
      "1.block.0.1.bias: False\n",
      "1.block.1.fc1.weight: False\n",
      "1.block.1.fc1.bias: False\n",
      "1.block.1.fc2.weight: False\n",
      "1.block.1.fc2.bias: False\n",
      "1.block.2.0.weight: False\n",
      "1.block.2.1.weight: False\n",
      "1.block.2.1.bias: False\n",
      "2.block.0.0.weight: False\n",
      "2.block.0.1.weight: False\n",
      "2.block.0.1.bias: False\n",
      "2.block.1.0.weight: False\n",
      "2.block.1.1.weight: False\n",
      "2.block.1.1.bias: False\n",
      "2.block.2.0.weight: False\n",
      "2.block.2.1.weight: False\n",
      "2.block.2.1.bias: False\n",
      "3.block.0.0.weight: False\n",
      "3.block.0.1.weight: False\n",
      "3.block.0.1.bias: False\n",
      "3.block.1.0.weight: False\n",
      "3.block.1.1.weight: False\n",
      "3.block.1.1.bias: False\n",
      "3.block.2.0.weight: False\n",
      "3.block.2.1.weight: False\n",
      "3.block.2.1.bias: False\n",
      "4.block.0.0.weight: False\n",
      "4.block.0.1.weight: False\n",
      "4.block.0.1.bias: False\n",
      "4.block.1.0.weight: False\n",
      "4.block.1.1.weight: False\n",
      "4.block.1.1.bias: False\n",
      "4.block.2.fc1.weight: False\n",
      "4.block.2.fc1.bias: False\n",
      "4.block.2.fc2.weight: False\n",
      "4.block.2.fc2.bias: False\n",
      "4.block.3.0.weight: False\n",
      "4.block.3.1.weight: False\n",
      "4.block.3.1.bias: False\n",
      "5.block.0.0.weight: False\n",
      "5.block.0.1.weight: False\n",
      "5.block.0.1.bias: False\n",
      "5.block.1.0.weight: False\n",
      "5.block.1.1.weight: False\n",
      "5.block.1.1.bias: False\n",
      "5.block.2.fc1.weight: False\n",
      "5.block.2.fc1.bias: False\n",
      "5.block.2.fc2.weight: False\n",
      "5.block.2.fc2.bias: False\n",
      "5.block.3.0.weight: False\n",
      "5.block.3.1.weight: False\n",
      "5.block.3.1.bias: False\n",
      "6.block.0.0.weight: False\n",
      "6.block.0.1.weight: False\n",
      "6.block.0.1.bias: False\n",
      "6.block.1.0.weight: False\n",
      "6.block.1.1.weight: False\n",
      "6.block.1.1.bias: False\n",
      "6.block.2.fc1.weight: False\n",
      "6.block.2.fc1.bias: False\n",
      "6.block.2.fc2.weight: False\n",
      "6.block.2.fc2.bias: False\n",
      "6.block.3.0.weight: False\n",
      "6.block.3.1.weight: False\n",
      "6.block.3.1.bias: False\n",
      "7.block.0.0.weight: False\n",
      "7.block.0.1.weight: False\n",
      "7.block.0.1.bias: False\n",
      "7.block.1.0.weight: False\n",
      "7.block.1.1.weight: False\n",
      "7.block.1.1.bias: False\n",
      "7.block.2.fc1.weight: False\n",
      "7.block.2.fc1.bias: False\n",
      "7.block.2.fc2.weight: False\n",
      "7.block.2.fc2.bias: False\n",
      "7.block.3.0.weight: False\n",
      "7.block.3.1.weight: False\n",
      "7.block.3.1.bias: False\n",
      "8.block.0.0.weight: False\n",
      "8.block.0.1.weight: False\n",
      "8.block.0.1.bias: False\n",
      "8.block.1.0.weight: False\n",
      "8.block.1.1.weight: False\n",
      "8.block.1.1.bias: False\n",
      "8.block.2.fc1.weight: False\n",
      "8.block.2.fc1.bias: False\n",
      "8.block.2.fc2.weight: False\n",
      "8.block.2.fc2.bias: False\n",
      "8.block.3.0.weight: False\n",
      "8.block.3.1.weight: False\n",
      "8.block.3.1.bias: False\n",
      "9.block.0.0.weight: False\n",
      "9.block.0.1.weight: False\n",
      "9.block.0.1.bias: False\n",
      "9.block.1.0.weight: False\n",
      "9.block.1.1.weight: False\n",
      "9.block.1.1.bias: False\n",
      "9.block.2.fc1.weight: False\n",
      "9.block.2.fc1.bias: False\n",
      "9.block.2.fc2.weight: False\n",
      "9.block.2.fc2.bias: False\n",
      "9.block.3.0.weight: False\n",
      "9.block.3.1.weight: False\n",
      "9.block.3.1.bias: False\n",
      "10.block.0.0.weight: False\n",
      "10.block.0.1.weight: False\n",
      "10.block.0.1.bias: False\n",
      "10.block.1.0.weight: False\n",
      "10.block.1.1.weight: False\n",
      "10.block.1.1.bias: False\n",
      "10.block.2.fc1.weight: False\n",
      "10.block.2.fc1.bias: False\n",
      "10.block.2.fc2.weight: False\n",
      "10.block.2.fc2.bias: False\n",
      "10.block.3.0.weight: False\n",
      "10.block.3.1.weight: False\n",
      "10.block.3.1.bias: False\n",
      "11.block.0.0.weight: False\n",
      "11.block.0.1.weight: False\n",
      "11.block.0.1.bias: False\n",
      "11.block.1.0.weight: False\n",
      "11.block.1.1.weight: False\n",
      "11.block.1.1.bias: False\n",
      "11.block.2.fc1.weight: False\n",
      "11.block.2.fc1.bias: False\n",
      "11.block.2.fc2.weight: False\n",
      "11.block.2.fc2.bias: False\n",
      "11.block.3.0.weight: False\n",
      "11.block.3.1.weight: False\n",
      "11.block.3.1.bias: False\n",
      "12.0.weight: False\n",
      "12.1.weight: False\n",
      "12.1.bias: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier Layer:\")\n",
    "for name, param in model.base_model.classifier.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")\n",
    "\n",
    "print(\"\\nConvolutional Layers:\")\n",
    "for name, param in model.base_model.features.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9be3e-9da7-4514-8022-a821f7657983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
